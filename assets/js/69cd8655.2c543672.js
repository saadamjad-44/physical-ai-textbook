"use strict";(globalThis.webpackChunkphysical_ai_book=globalThis.webpackChunkphysical_ai_book||[]).push([[537],{8540(e,n,o){o.r(n),o.d(n,{assets:()=>l,contentTitle:()=>c,default:()=>h,frontMatter:()=>s,metadata:()=>i,toc:()=>r});const i=JSON.parse('{"id":"module-4-vla/vla-basics","title":"Module 4: Vision-Language-Action (VLA)","description":"The Convergence of LLMs and Robotics","source":"@site/docs/module-4-vla/01-vla-basics.md","sourceDirName":"module-4-vla","slug":"/module-4-vla/vla-basics","permalink":"/physical-ai-textbook/docs/module-4-vla/vla-basics","draft":false,"unlisted":false,"tags":[],"version":"current","sidebarPosition":1,"frontMatter":{},"sidebar":"courseSidebar","previous":{"title":"Module 3: The AI-Robot Brain (NVIDIA Isaac\u2122)","permalink":"/physical-ai-textbook/docs/module-3-nvidia-isaac/nvidia-isaac"}}');var t=o(4848),a=o(8453);const s={},c="Module 4: Vision-Language-Action (VLA)",l={},r=[{value:"The Convergence of LLMs and Robotics",id:"the-convergence-of-llms-and-robotics",level:2},{value:"Voice-to-Action with OpenAI Whisper",id:"voice-to-action-with-openai-whisper",level:2},{value:"Cognitive Planning",id:"cognitive-planning",level:2},{value:"Capstone Project: The Autonomous Humanoid",id:"capstone-project-the-autonomous-humanoid",level:2}];function d(e){const n={h1:"h1",h2:"h2",header:"header",li:"li",ol:"ol",p:"p",...(0,a.R)(),...e.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(n.header,{children:(0,t.jsx)(n.h1,{id:"module-4-vision-language-action-vla",children:"Module 4: Vision-Language-Action (VLA)"})}),"\n",(0,t.jsx)(n.h2,{id:"the-convergence-of-llms-and-robotics",children:"The Convergence of LLMs and Robotics"}),"\n",(0,t.jsx)(n.p,{children:"The future of robotics involves robots that can understand natural language and translate it into physical actions. This is referred to as Vision-Language-Action (VLA) modeling."}),"\n",(0,t.jsx)(n.h2,{id:"voice-to-action-with-openai-whisper",children:"Voice-to-Action with OpenAI Whisper"}),"\n",(0,t.jsx)(n.p,{children:"We can use OpenAI Whisper to transcribe human voice commands into text, which can then be processed by an LLM-based agent."}),"\n",(0,t.jsx)(n.h2,{id:"cognitive-planning",children:"Cognitive Planning"}),"\n",(0,t.jsx)(n.p,{children:'An LLM can act as a high-level planner, breaking down a command like "Go to the kitchen and bring me a cup" into a sequence of ROS 2 actions:'}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsx)(n.li,{children:"Navigate to 'kitchen' waypoint."}),"\n",(0,t.jsx)(n.li,{children:"Search for 'cup' using Computer Vision."}),"\n",(0,t.jsx)(n.li,{children:"Plan grasp trajectory."}),"\n",(0,t.jsx)(n.li,{children:"Execute grasp."}),"\n",(0,t.jsx)(n.li,{children:"Navigate back to 'user'."}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"capstone-project-the-autonomous-humanoid",children:"Capstone Project: The Autonomous Humanoid"}),"\n",(0,t.jsx)(n.p,{children:"In the final project, you will implement a simulated humanoid that receives a voice command, plans its path, navigates obstacles, and interacts with objects."})]})}function h(e={}){const{wrapper:n}={...(0,a.R)(),...e.components};return n?(0,t.jsx)(n,{...e,children:(0,t.jsx)(d,{...e})}):d(e)}},8453(e,n,o){o.d(n,{R:()=>s,x:()=>c});var i=o(6540);const t={},a=i.createContext(t);function s(e){const n=i.useContext(a);return i.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function c(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(t):e.components||t:s(e.components),i.createElement(a.Provider,{value:n},e.children)}}}]);